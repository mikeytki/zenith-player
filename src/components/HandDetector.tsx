import React, { useEffect, useRef, useState } from 'react';
import { FilesetResolver, GestureRecognizer } from "@mediapipe/tasks-vision";
import { usePlayerStore, GestureType } from '../store/usePlayerStore';
import { useShallow } from 'zustand/react/shallow';

// å¹³æ»‘ç³»æ•°
const LERP_FACTOR = 0.15;
// åŠ¨ä½œå†·å´æ—¶é—´ (æ¯«ç§’)ï¼Œé˜²æ­¢è¯¯è§¦è¿ç‚¹
const ACTION_COOLDOWN = 1000;
// æåˆæ£€æµ‹é˜ˆå€¼
const PINCH_THRESHOLD = 0.07;
// æ‰‹åŠ¿ç¡®è®¤å¸§æ•° - éœ€è¦è¿ç»­æ£€æµ‹åˆ°ç›¸åŒæ‰‹åŠ¿æ‰è§¦å‘
const GESTURE_CONFIRM_FRAMES = 5; // é™ä½åˆ°5å¸§ï¼Œæ›´å¿«å“åº” 

const HandDetector: React.FC = () => {
  const videoRef = useRef<HTMLVideoElement>(null);
  
  // 1. è·å–è¾“å…¥æ¨¡å¼å’Œ Setter (ç”¨äºè¯†åˆ«é€»è¾‘)
  const { 
    inputMode, 
    setGesture, 
    setCursorPosition, 
    setCameraStatus, 
    setInputMode 
  } = usePlayerStore(useShallow(state => ({
    inputMode: state.inputMode,
    setGesture: state.setGesture,
    setCursorPosition: state.setCursorPosition,
    setCameraStatus: state.setCameraStatus,
    setInputMode: state.setInputMode,
  })));

  // 2. [æ–°å¢] è·å–æ’­æ”¾æ§åˆ¶åŠ¨ä½œ (ç”¨äºæ‰§è¡Œé€»è¾‘)
  const {
    currentGesture, // ç›‘å¬å½“å‰æ‰‹åŠ¿
    play,
    pause,
    nextSong
  } = usePlayerStore(useShallow(state => ({
    currentGesture: state.currentGesture,
    play: state.play,
    pause: state.pause,
    nextSong: state.nextSong
  })));

  // Refs
  const lastCursorRef = useRef({ x: 0, y: 0 });
  const recognizerRef = useRef<GestureRecognizer | null>(null);
  const requestRef = useRef<number | null>(null);
  const lastActionTimeRef = useRef<number>(0); // åŠ¨ä½œå†·å´è®¡æ—¶å™¨

  // æ‰‹åŠ¿ç¡®è®¤æœºåˆ¶
  const gestureHistoryRef = useRef<GestureType[]>([]);
  const confirmedGestureRef = useRef<GestureType>('NONE');

  const [isModelLoaded, setIsModelLoaded] = useState(false);

  // === æ ¸å¿ƒé€»è¾‘ï¼šç›‘å¬æ‰‹åŠ¿å˜åŒ–å¹¶æ‰§è¡Œå‘½ä»¤ ===
  useEffect(() => {
    if (inputMode !== 'HAND') return;

    const now = Date.now();
    // æ£€æŸ¥å†·å´æ—¶é—´
    if (now - lastActionTimeRef.current < ACTION_COOLDOWN) return;

    // åªå“åº”å·²ç¡®è®¤çš„æ‰‹åŠ¿
    const gesture = confirmedGestureRef.current;

    if (gesture === 'PINCH') {
        console.log("ğŸ‘Œ Gesture Trigger: NEXT SONG");
        nextSong();
        lastActionTimeRef.current = now;
        confirmedGestureRef.current = 'NONE';
        gestureHistoryRef.current = []; // æ¸…ç©ºå†å²
    }
    else if (gesture === 'OPEN') {
        // å¼ å¼€æ‰‹æŒ = æ’­æ”¾ï¼ˆæ— è®ºå½“å‰çŠ¶æ€ï¼‰
        console.log("ğŸ–ï¸ Gesture Trigger: PLAY");
        play();
        lastActionTimeRef.current = now;
        confirmedGestureRef.current = 'NONE';
        gestureHistoryRef.current = [];
    }
    else if (gesture === 'FIST') {
        // æ¡æ‹³ = æš‚åœï¼ˆæ— è®ºå½“å‰çŠ¶æ€ï¼‰
        console.log("âœŠ Gesture Trigger: PAUSE");
        pause();
        lastActionTimeRef.current = now;
        confirmedGestureRef.current = 'NONE';
        gestureHistoryRef.current = [];
    }
  }, [currentGesture, inputMode, nextSong, play, pause]);

  // === ä»¥ä¸‹ä¸º MediaPipe åˆå§‹åŒ–ä¸å¾ªç¯é€»è¾‘ (ä¿æŒä¸å˜) ===

  useEffect(() => {
    const initModel = async () => {
      try {
        const vision = await FilesetResolver.forVisionTasks(
          "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm"
        );
        const recognizer = await GestureRecognizer.createFromOptions(vision, {
          baseOptions: {
            modelAssetPath: "https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task",
            delegate: "GPU"
          },
          runningMode: "VIDEO",
          numHands: 1
        });
        recognizerRef.current = recognizer;
        setIsModelLoaded(true);
        console.log("âœ… MediaPipe Model Loaded");
      } catch (error) {
        console.error("MediaPipe load error:", error);
      }
    };
    initModel();
  }, []);

  useEffect(() => {
    if (!isModelLoaded || inputMode !== 'HAND') {
        if (inputMode !== 'HAND' && videoRef.current && videoRef.current.srcObject) {
            const stream = videoRef.current.srcObject as MediaStream;
            stream.getTracks().forEach(track => track.stop());
            videoRef.current.srcObject = null;
        }
        if (requestRef.current) cancelAnimationFrame(requestRef.current);
        return;
    }

    const enableCam = async () => {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });
        if (videoRef.current) {
          videoRef.current.srcObject = stream;
          videoRef.current.onloadeddata = () => {
            setCameraStatus(true);
            predictWebcam();
          };
        }
      } catch (err) {
        console.error("Camera denied:", err);
        setCameraStatus(false);
        setInputMode('MOUSE');
      }
    };

    enableCam();

    return () => {
      if (requestRef.current) cancelAnimationFrame(requestRef.current);
      if (videoRef.current && videoRef.current.srcObject) {
        const stream = videoRef.current.srcObject as MediaStream;
        stream.getTracks().forEach(track => track.stop());
      }
    };
  }, [isModelLoaded, inputMode, setCameraStatus, setInputMode]);

  const predictWebcam = () => {
    if (!videoRef.current || !recognizerRef.current || inputMode !== 'HAND') return;

    const nowInMs = Date.now();
    if (videoRef.current.videoWidth > 0 && videoRef.current.videoHeight > 0) {
        const results = recognizerRef.current.recognizeForVideo(videoRef.current, nowInMs);

        if (results.gestures.length > 0) {
          const mediapipeGesture = results.gestures[0][0].categoryName;
          const confidence = results.gestures[0][0].score;
          const landmarks = results.landmarks[0];

          // è·å–æ‰‹æŒ‡ä½ç½®ï¼ˆç”¨äºå…‰æ ‡ï¼‰
          const thumbTip = landmarks[4];
          const indexTip = landmarks[8];

          let myGesture: GestureType = 'NONE';

          // åªæ¥å—é«˜ç½®ä¿¡åº¦çš„æ‰‹åŠ¿ (>0.6)
          if (confidence > 0.6) {
            const distance = Math.hypot(thumbTip.x - indexTip.x, thumbTip.y - indexTip.y);

            if (distance < PINCH_THRESHOLD) {
              myGesture = 'PINCH';
            } else {
              // å¦‚æœä¸æ˜¯æåˆï¼Œå†æ£€æµ‹å…¶ä»–æ‰‹åŠ¿
              if (mediapipeGesture === 'Open_Palm') myGesture = 'OPEN';
              else if (mediapipeGesture === 'Closed_Fist') myGesture = 'FIST';
              else if (mediapipeGesture === 'Victory') myGesture = 'POINT';
            }
          }

          // æ‰‹åŠ¿ç¡®è®¤æœºåˆ¶ï¼šéœ€è¦è¿ç»­å¤šå¸§æ£€æµ‹åˆ°ç›¸åŒæ‰‹åŠ¿
          gestureHistoryRef.current.push(myGesture);
          if (gestureHistoryRef.current.length > GESTURE_CONFIRM_FRAMES) {
            gestureHistoryRef.current.shift();
          }

          // æ£€æŸ¥æœ€è¿‘çš„å¸§æ˜¯å¦éƒ½æ˜¯åŒä¸€ä¸ªæ‰‹åŠ¿
          if (gestureHistoryRef.current.length === GESTURE_CONFIRM_FRAMES) {
            const allSame = gestureHistoryRef.current.every(g => g === myGesture);
            if (allSame && myGesture !== 'NONE' && myGesture !== 'POINT') {
              // åªæœ‰å½“ç¡®è®¤æ‰‹åŠ¿ä¸å½“å‰ä¸åŒæ—¶æ‰æ›´æ–°
              if (confirmedGestureRef.current !== myGesture) {
                confirmedGestureRef.current = myGesture;
                setGesture(myGesture);
              }
            } else if (myGesture === 'NONE' || myGesture === 'POINT') {
              // NONE å’Œ POINT ç«‹å³æ›´æ–°ï¼ˆç”¨äºå…‰æ ‡æ§åˆ¶ï¼‰
              setGesture(myGesture);
            }
          }

          // æ›´æ–°å…‰æ ‡ä½ç½®
          const rawX = indexTip.x;
          const rawY = indexTip.y;
          const mirroredX = 1 - rawX;
          const targetX = (mirroredX * 2) - 1;
          const targetY = -(rawY * 2) + 1;

          const smoothX = lastCursorRef.current.x + (targetX - lastCursorRef.current.x) * LERP_FACTOR;
          const smoothY = lastCursorRef.current.y + (targetY - lastCursorRef.current.y) * LERP_FACTOR;

          lastCursorRef.current = { x: smoothX, y: smoothY };
          setCursorPosition(smoothX, smoothY);
        } else {
          setGesture('NONE');
          gestureHistoryRef.current = [];
          confirmedGestureRef.current = 'NONE';
        }
    }

    // åªåœ¨ HAND æ¨¡å¼ä¸‹ç»§ç»­å¾ªç¯
    if (inputMode === 'HAND') {
      requestRef.current = requestAnimationFrame(predictWebcam);
    }
  };

  return (
    <video 
      ref={videoRef} 
      autoPlay 
      playsInline 
      muted
      className="hidden fixed top-0 left-0 w-32 h-32 opacity-0 pointer-events-none" 
    />
  );
};

export default HandDetector;